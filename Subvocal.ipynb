{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Classes\n",
    "\n",
    "First we get all of our helper modules. The prepare_EMG module will prepare the EMG data for phoneme recognition. The prepare_outputs module will prepare our target labels and align them with our EMG data. The module 'prepare_data' will help us read data from CSV into a dataframe. Finally, 'vis' will help visualize EMG data in both time and frequency domains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.896547109208\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import prepare_EMG, prepare_outputs, prepare_data, vis\n",
    "# autodetector = Output_Prep.detector\n",
    "EMG_Prep = prepare_EMG.EMG_preparer(window_size=60.0)\n",
    "# Output_Prep = prepare_outputs.output_preparer(subvocal_detector = autodetector, window_size=30.0)\n",
    "Output_Prep = prepare_outputs.output_preparer(window_size=60.0,do_grid_search=False)\n",
    "\n",
    "Data_Prep = prepare_data.data_preparer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling the Data\n",
    "\n",
    "First, we need to visualize a few EMG voltage graphs to find some sections that most likely contain no subvocalization. Then, we'll need to find some regions that almost certainly do. These two classes of EMG readouts will serve to train an identifier to help us automatically label EMG windows with phonemes. The model used here will most likely be an SVC, inside \"prepare_outputs\". It will process each EMG window in order, and when it finds one that most likely contains subvocalization, it applies the next phoneme as that window's label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_1 = Data_Prep.load('Sat Mar  4 00:44:23 2017')\n",
    "data_2 = Data_Prep.load('Sat Mar  4 00:45:02 2017')\n",
    "data_3 = Data_Prep.load('Sat Mar  4 00:45:47 2017')\n",
    "data_4 = Data_Prep.load('Sat Mar  4 00:47:01 2017')\n",
    "data_5 = Data_Prep.load('Sat Mar  4 00:47:36 2017')\n",
    "data_6 = Data_Prep.load('Sat Mar  4 00:48:09 2017')\n",
    "data_7 = Data_Prep.load('Sat Mar  4 00:49:05 2017')\n",
    "data_8 = Data_Prep.load('Sat Mar  4 00:49:41 2017')\n",
    "data_9 = Data_Prep.load('Sat Mar  4 00:50:22 2017')\n",
    "data_10 = Data_Prep.load('Sat Mar  4 00:51:17 2017')\n",
    "data_11 = Data_Prep.load('Sat Mar  4 00:52:02 2017')\n",
    "data_12 = Data_Prep.load('Sat Mar  4 00:52:38 2017')\n",
    "data_13 = Data_Prep.load('Sat Mar  4 00:53:24 2017')\n",
    "data_14 = Data_Prep.load('Sat Mar  4 00:53:51 2017')\n",
    "data_15 = Data_Prep.load('Sat Mar  4 00:54:25 2017')\n",
    "data_16 = Data_Prep.load('Sat Mar  4 00:54:57 2017')\n",
    "data_17 = Data_Prep.load('Sat Mar  4 00:56:01 2017')\n",
    "data_18 = Data_Prep.load('Sat Mar  4 00:56:35 2017')\n",
    "data_19 = Data_Prep.load('Sat Mar  4 00:57:21 2017')\n",
    "data_20 = Data_Prep.load('Sat Mar  4 00:57:49 2017')\n",
    "data_21 = Data_Prep.load('Sat Mar  4 00:58:59 2017')\n",
    "data_22 = Data_Prep.load('Sat Mar  4 00:59:53 2017')\n",
    "\n",
    "data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10, data_11, data_12, data_13, data_14, data_15, data_16, data_17, data_18, data_19, data_20, data_21, data_22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned Data shape: (5300, 60) Trans labels shape: (5300, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "%autoreload 2\n",
    "\n",
    "num_files = len(data_list)\n",
    "labels_frame = pandas.read_csv('austen_subvocal.csv')\n",
    "trans_labels = Output_Prep.transform(labels_frame.iloc[0][0])\n",
    "data_1_proc = EMG_Prep.process(data_1)\n",
    "aligned_data, trans_labels= Output_Prep.zip(data_1_proc, trans_labels, repeat=3)\n",
    "\n",
    "for file in range(1, num_files):\n",
    "    trans_labels_iter = Output_Prep.transform(labels_frame.iloc[file][0])\n",
    "    data_proc_iter = EMG_Prep.process(data_list[file])\n",
    "    aligned_data_iter, trans_labels_iter = Output_Prep.zip(data_proc_iter, trans_labels_iter, repeat=3)\n",
    "\n",
    "    aligned_data = aligned_data.append(aligned_data_iter)\n",
    "    trans_labels = trans_labels.append(trans_labels_iter)\n",
    "    \n",
    "print('Aligned Data shape:',aligned_data.shape,'Trans labels shape:',trans_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AF Extractor Models\n",
    "\n",
    "These models will be optimized for extracting AF's from the data, before passing those AF's onto an MLPC for identifying the most likely phoneme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number layer sizes: 5 here be layer sizes [(30, 30), (60, 60), (90, 90), (120, 120), (150, 150)]\n"
     ]
    }
   ],
   "source": [
    "# Prepare lists of parameters for our GridSearch\n",
    "# First, our layer sizes\n",
    "layer_sizes = []\n",
    "for i in range(2,3):\n",
    "    for j in range(0,180,30):\n",
    "        if j:\n",
    "            tup = []\n",
    "            for k in range(i):\n",
    "                tup.append(j)\n",
    "            layer_sizes.append(tuple(tup))\n",
    "print('number layer sizes:',len(layer_sizes),'here be layer sizes',layer_sizes)\n",
    "\n",
    "# Next, our alpha values\n",
    "alphas = [0.0000001,1,1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing GridSearch and Assesing Stock MLPC as AF extractor models\n",
    "\n",
    "We setup the objects for performing gridsearch on each one of the Articulatory Feature Extractor models. We also train untuned, stock MLPC models to serve as a performance baseline. We will compare the performance of these baseline, untuned models to our gridsearched models to determine whether gridsearch has in fact improved the model parameters for each AF extractor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manner score: 0.403773584906 place score: 0.283018867925 height score: 0.491823899371 vowel score: 0.592452830189\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier as MLPC\n",
    "# Import other models to try for feature extraction\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "import copy\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(aligned_data, trans_labels, test_size=0.15, random_state=12)\n",
    "\n",
    "combined_features = FeatureUnion([\n",
    "    ('pca',PCA(random_state=18)),\n",
    "    ('kbest',SelectKBest(k=1))\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', combined_features),\n",
    "    ('model', MLPC(random_state=12))\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'features__pca__n_components':[0.5,0.66,0.9],\n",
    "    'features__kbest__k':[1,2,3],\n",
    "    'model__solver':['adam'],\n",
    "    'model__hidden_layer_sizes':layer_sizes,\n",
    "    'model__activation':['relu'],\n",
    "    'model__alpha': alphas,\n",
    "    'model__max_iter':[200]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, n_jobs=-1)\n",
    "\n",
    "manner_classifier = MLPC(solver='adam',random_state=3)\n",
    "manner_classifier.fit(X_train, y_train['manner'])\n",
    "m_score = manner_classifier.score(X_test, y_test['manner'])\n",
    "\n",
    "place_classifier = MLPC(solver='adam',random_state=6)\n",
    "place_classifier.fit(X_train, y_train['place'])\n",
    "p_score = place_classifier.score(X_test, y_test['place'])\n",
    "\n",
    "height_classifier = MLPC(solver='adam',random_state=9)\n",
    "height_classifier.fit(X_train, y_train['height'])\n",
    "h_score = height_classifier.score(X_test, y_test['height'])\n",
    "\n",
    "vowel_classifier = MLPC(solver='adam',random_state=12)\n",
    "vowel_classifier.fit(X_train, y_train['vowel'])\n",
    "v_score = vowel_classifier.score(X_test, y_test['vowel'])\n",
    "\n",
    "print('manner score:',m_score,'place score:',p_score,'height score:',h_score,'vowel score:',v_score)\n",
    "# print(data_1_proc.head(50), trans_labels['manner'].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manner_classifier2 = copy.deepcopy(grid_search)\n",
    "manner_classifier2.fit(aligned_data, trans_labels['manner'])\n",
    "m_score2 = manner_classifier2.score(aligned_data, trans_labels['manner'])\n",
    "\n",
    "print('manner score:',m_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "place_classifier2 = copy.deepcopy(grid_search)\n",
    "place_classifier2.fit(aligned_data, trans_labels['place'])\n",
    "p_score2 = place_classifier2.score(aligned_data, trans_labels['place'])\n",
    "\n",
    "print('place score:',p_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height_classifier2 = copy.deepcopy(grid_search)\n",
    "height_classifier2.fit(aligned_data, trans_labels['height'])\n",
    "h_score2 = height_classifier2.score(aligned_data, trans_labels['height'])\n",
    "\n",
    "print('vowel score:',h_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n",
      "/home/brian/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vowel score: 0.610566037736\n"
     ]
    }
   ],
   "source": [
    "vowel_classifier2 = copy.deepcopy(grid_search)\n",
    "vowel_classifier2.fit(aligned_data, trans_labels['vowel'])\n",
    "v_score2 = vowel_classifier2.score(aligned_data, trans_labels['vowel'])\n",
    "\n",
    "print('vowel score:',v_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manner score: 0.405031446541\n"
     ]
    }
   ],
   "source": [
    "# Experiment with PCA here\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "manner_union = FeatureUnion([('pca',PCA(n_components=0.03)),('kbest',SelectKBest(k=1))])\n",
    "manner_reduced_data = manner_union.fit_transform(aligned_data, trans_labels['manner'])\n",
    "manner_X_train, manner_X_test, manner_y_train, manner_y_test = train_test_split(manner_reduced_data, trans_labels, test_size=0.15, random_state=12)\n",
    "\n",
    "\n",
    "manner_classifier3 = MLPC(solver='adam',alpha=1000,hidden_layer_sizes=(1),random_state=3,max_iter=300)\n",
    "manner_classifier3.fit(manner_X_train, manner_y_train['manner'])\n",
    "m_score3 = manner_classifier3.score(manner_X_test, manner_y_test['manner'])\n",
    "print('manner score:',m_score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "place classifier score: 0.161006289308\n"
     ]
    }
   ],
   "source": [
    "place_union = FeatureUnion([('pca',PCA(n_components=0.9)),('kbest',SelectKBest(k=1))])\n",
    "place_reduced_data = place_union.fit_transform(aligned_data, trans_labels['place'])\n",
    "place_X_train, place_X_test, place_y_train, place_y_test = train_test_split(place_reduced_data, trans_labels, test_size=0.15, random_state=12)\n",
    "\n",
    "\n",
    "place_classifier3 = MLPC(solver='adam',alpha=0.00001,hidden_layer_sizes=(120,120),random_state=6,max_iter=300)\n",
    "place_classifier3.fit(place_X_train, place_y_train['place'])\n",
    "p_score3 = place_classifier3.score(place_X_test, place_y_test['place'])\n",
    "print('place classifier score:',p_score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height score: 0.353459119497\n"
     ]
    }
   ],
   "source": [
    "height_union = FeatureUnion([('pca',PCA(n_components=0.9)),('kbest',SelectKBest(k=1))])\n",
    "height_reduced_data = height_union.fit_transform(aligned_data, trans_labels['height'])\n",
    "height_X_train, height_X_test, height_y_train, height_y_test = train_test_split(height_reduced_data, trans_labels, test_size=0.15, random_state=12)\n",
    "\n",
    "\n",
    "height_classifier3 = MLPC(solver='adam',alpha=0.00001,hidden_layer_sizes=(180,180),random_state=12,max_iter=300)\n",
    "height_classifier3.fit(height_X_train, height_y_train['height'])\n",
    "h_score3 = height_classifier3.score(height_X_test, height_y_test['height'])\n",
    "print('height score:',h_score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vowel score: 0.522012578616\n"
     ]
    }
   ],
   "source": [
    "vowel_union = FeatureUnion([('pca',PCA(n_components=0.9)),('kbest',SelectKBest(k=1))])\n",
    "vowel_reduced_data = vowel_union.fit_transform(aligned_data, trans_labels['vowel'])\n",
    "vowel_X_train, vowel_X_test, vowel_y_train, vowel_y_test = train_test_split(vowel_reduced_data, trans_labels, test_size=0.15, random_state=12)\n",
    "\n",
    "\n",
    "vowel_classifier3 = MLPC(solver='adam',alpha=0.00001,hidden_layer_sizes=(180,180),random_state=12,max_iter=300)\n",
    "vowel_classifier3.fit(vowel_X_train, vowel_y_train['vowel'])\n",
    "v_score3 = vowel_classifier3.score(vowel_X_test, vowel_y_test['vowel'])\n",
    "print('vowel score:',v_score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.016666666666666666, 0.016666666666666666, 0.033333333333333333, 0.033333333333333333, 0.050000000000000003, 0.050000000000000003, 0.066666666666666666, 0.066666666666666666, 0.083333333333333329, 0.083333333333333329, 0.10000000000000001, 0.10000000000000001, 0.11666666666666667, 0.11666666666666667, 0.13333333333333333, 0.13333333333333333, 0.14999999999999999, 0.14999999999999999, 0.16666666666666666, 0.16666666666666666, 0.18333333333333332, 0.18333333333333332, 0.20000000000000001, 0.20000000000000001, 0.21666666666666667, 0.21666666666666667, 0.23333333333333334, 0.23333333333333334, 0.25, 0.25, 0.26666666666666666, 0.26666666666666666, 0.28333333333333333, 0.28333333333333333, 0.29999999999999999, 0.29999999999999999, 0.31666666666666665, 0.31666666666666665, 0.33333333333333331, 0.33333333333333331, 0.34999999999999998, 0.34999999999999998, 0.36666666666666664, 0.36666666666666664, 0.38333333333333336, 0.38333333333333336, 0.40000000000000002, 0.40000000000000002, 0.41666666666666669, 0.41666666666666669, 0.43333333333333335, 0.43333333333333335, 0.45000000000000001, 0.45000000000000001, 0.46666666666666667, 0.46666666666666667, 0.48333333333333334, 0.48333333333333334, 0.5, 'manner', 'place', 'height', 'vowel']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'no' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-c009f98b4731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# print(phoneme_inputs.head(100))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mphoneme_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mno\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphoneme_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no found in phoneme_labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mphoneme_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'no' is not defined"
     ]
    }
   ],
   "source": [
    "# print(aligned_data.head(),trans_labels.head())\n",
    "\n",
    "# phoneme_inputs = pandas.concat([aligned_data,trans_labels['manner'],trans_labels['place'],trans_labels['height'],trans_labels['vowel']],axis=1,join='outer')\n",
    "cols = list(aligned_data.axes[1]) + list(trans_labels.axes[1])\n",
    "# print(cols)\n",
    "\n",
    "phoneme_inputs = pandas.DataFrame(columns=cols)\n",
    "# print(phoneme_inputs)\n",
    "for row in range(aligned_data.shape[0]):\n",
    "    new_row = aligned_data.iloc[row].append(trans_labels.iloc[row])\n",
    "    new_row = new_row.rename(trans_labels.iloc[row].name)\n",
    "#     print(new_row.name)\n",
    "    phoneme_inputs = phoneme_inputs.append(new_row)\n",
    "\n",
    "# print(phoneme_inputs.head(100))\n",
    "phoneme_labels = trans_labels.axes[0]\n",
    "# if 'no' in phoneme_labels:\n",
    "#     print('no found in phoneme_labels')\n",
    "phoneme_classifier = MLPC(solver='adam',hidden_layer_sizes=(90,90),random_state=6, max_iter=300)\n",
    "phoneme_classifier.fit(phoneme_inputs, phoneme_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'no'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-7348cf0ec22b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no found in phoneme_inputs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mphoneme_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.000001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mphoneme_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoneme_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoneme_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \"\"\"\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    328\u001b[0m                              hidden_layer_sizes)\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 902\u001b[0;31m                          multi_output=True)\n\u001b[0m\u001b[1;32m    903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;31m# make sure we actually converted to numeric:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"O\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'no'"
     ]
    }
   ],
   "source": [
    "phoneme_classifier = MLPC(solver='adam',alpha=0.000001,hidden_layer_sizes=(240,240),random_state=6, max_iter=300)\n",
    "phoneme_classifier.fit(phoneme_inputs, phoneme_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0922641509434\n"
     ]
    }
   ],
   "source": [
    "pho_score = phoneme_classifier.score(aligned_data, phoneme_labels)\n",
    "print(pho_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 0 0 1 1 0 0 1 0 0 0]\n",
      " [1 1 1 0 0 0 1 0 0 0 1 0 0 0 0]\n",
      " [0 0 1 1 1 1 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 1 0 0 1 0 0 0 1 1]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'no'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-abd0fbced3b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOHE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \"\"\"\n\u001b[1;32m   1901\u001b[0m         return _transform_selected(X, self._fit_transform,\n\u001b[0;32m-> 1902\u001b[0;31m                                    self.categorical_features, copy=True)\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36m_transform_selected\u001b[0;34m(X, transform, selected, copy)\u001b[0m\n\u001b[1;32m   1695\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msparse\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m     \"\"\"\n\u001b[0;32m-> 1697\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mselected\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    380\u001b[0m                                       force_all_finite)\n\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'no'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder as LE\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "from sklearn.preprocessing import MultiLabelBinarizer as MLB\n",
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "\n",
    "binarizer = MLB()\n",
    "binarized_labels = binarizer.fit_transform(trans_labels)\n",
    "print(binarized_labels)\n",
    "\n",
    "encoder = OHE()\n",
    "encoded = encoder.fit_transform(trans_labels)\n",
    "encoded = encoded.toarray()\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
